#!/usr/bin/env python3
"""
ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ë¡œ ë¦¬ì„œì¹˜ ì—”ì§„ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import urllib.request
import urllib.error
from html.parser import HTMLParser
import re
from datetime import datetime
from typing import Dict, List, Any

class SimpleHTMLParser(HTMLParser):
    """ê°„ë‹¨í•œ HTML íŒŒì„œ"""
    
    def __init__(self):
        super().__init__()
        self.text_content = []
        self.current_tag = None
        self.links = []
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        if tag == 'a':
            for attr_name, attr_value in attrs:
                if attr_name == 'href' and attr_value:
                    self.links.append(attr_value)
    
    def handle_endtag(self, tag):
        self.current_tag = None
        
    def handle_data(self, data):
        if self.current_tag not in ['script', 'style', 'head']:
            clean_data = data.strip()
            if clean_data and len(clean_data) > 2:
                self.text_content.append(clean_data)

def fetch_website_content(url: str) -> Dict[str, Any]:
    """ì›¹ì‚¬ì´íŠ¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (urllib ì‚¬ìš©)"""
    try:
        print(f"ðŸŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘: {url}")
        
        # User-Agent í—¤ë” ì¶”ê°€
        req = urllib.request.Request(
            url, 
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        )
        
        with urllib.request.urlopen(req, timeout=10) as response:
            html_content = response.read().decode('utf-8', errors='ignore')
            
        print(f"âœ… HTML ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(html_content):,}ìž")
        
        # HTML íŒŒì‹±
        parser = SimpleHTMLParser()
        parser.feed(html_content)
        
        text_content = ' '.join(parser.text_content)
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text_content = re.sub(r'\s+', ' ', text_content)
        text_content = text_content[:5000]  # ì²˜ìŒ 5000ìžë§Œ
        
        return {
            'url': url,
            'content': text_content,
            'links': parser.links[:20],  # ì²˜ìŒ 20ê°œ ë§í¬ë§Œ
            'length': len(text_content),
            'status': 'success'
        }
        
    except urllib.error.HTTPError as e:
        print(f"âŒ HTTP ì˜¤ë¥˜: {e.code} - {e.reason}")
        return {'url': url, 'error': f'HTTP {e.code}', 'status': 'error'}
    except urllib.error.URLError as e:
        print(f"âŒ URL ì˜¤ë¥˜: {e.reason}")
        return {'url': url, 'error': str(e.reason), 'status': 'error'}
    except Exception as e:
        print(f"âŒ ê¸°íƒ€ ì˜¤ë¥˜: {str(e)}")
        return {'url': url, 'error': str(e), 'status': 'error'}

def extract_company_info_basic(content: str, company_name: str) -> Dict[str, Any]:
    """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (ê·œì¹™ ê¸°ë°˜)"""
    
    content_lower = content.lower()
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
    extracted_info = {
        'company_name': company_name,
        'business_keywords': [],
        'technology_keywords': [],
        'pipeline_mentions': [],
        'partnership_mentions': [],
        'contact_info': {}
    }
    
    # ë°”ì´ì˜¤/ì œì•½ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
    bio_keywords = [
        'adc', 'antibody', 'í•­ì²´', 'drug', 'ì˜ì•½í’ˆ', 'ì¹˜ë£Œì œ', 
        'pipeline', 'íŒŒì´í”„ë¼ì¸', 'clinical', 'ìž„ìƒ', 'phase',
        'biotech', 'ë°”ì´ì˜¤', 'pharmaceutical', 'ì œì•½'
    ]
    
    found_keywords = []
    for keyword in bio_keywords:
        if keyword in content_lower:
            found_keywords.append(keyword)
    
    extracted_info['business_keywords'] = found_keywords
    
    # íŒŒì´í”„ë¼ì¸ ì½”ë“œ ì°¾ê¸° (LCB##, YH#### ë“±)
    pipeline_patterns = [
        r'[A-Z]{2,3}[B]?\d{1,4}',  # LCB11, YH1234 ë“±
        r'[A-Z]+-\d{1,4}',         # AB-123 ë“±
    ]
    
    pipeline_codes = []
    for pattern in pipeline_patterns:
        matches = re.findall(pattern, content)
        pipeline_codes.extend(matches)
    
    extracted_info['pipeline_mentions'] = list(set(pipeline_codes))  # ì¤‘ë³µ ì œê±°
    
    # ì´ë©”ì¼, ì „í™”ë²ˆí˜¸ ì°¾ê¸°
    email_pattern = r'[\w\.-]+@[\w\.-]+\.\w+'
    phone_pattern = r'[\+]?[1-9]?[\d\s\-\(\)]{8,15}'
    
    emails = re.findall(email_pattern, content)
    phones = re.findall(phone_pattern, content)
    
    extracted_info['contact_info'] = {
        'emails': emails[:3],  # ì²˜ìŒ 3ê°œë§Œ
        'phones': [p.strip() for p in phones[:3] if len(p.strip()) > 8]
    }
    
    return extracted_info

def analyze_investment_signals(content: str) -> Dict[str, Any]:
    """íˆ¬ìž ì‹ í˜¸ ë¶„ì„"""
    
    content_lower = content.lower()
    
    # ê¸ì •ì  ì‹ í˜¸ í‚¤ì›Œë“œ
    positive_keywords = [
        'approval', 'ìŠ¹ì¸', 'partnership', 'íŒŒíŠ¸ë„ˆì‹­', 'licensing', 'ë¼ì´ì„ ì‹±',
        'breakthrough', 'positive', 'ê¸ì •ì ', 'success', 'ì„±ê³µ', 'milestone', 'ë§ˆì¼ìŠ¤í†¤',
        'funding', 'íˆ¬ìž', 'revenue', 'ë§¤ì¶œ', 'growth', 'ì„±ìž¥'
    ]
    
    # ë¶€ì •ì  ì‹ í˜¸ í‚¤ì›Œë“œ
    negative_keywords = [
        'failure', 'ì‹¤íŒ¨', 'delay', 'ì§€ì—°', 'suspend', 'ì¤‘ë‹¨', 'risk', 'ë¦¬ìŠ¤í¬',
        'concern', 'ìš°ë ¤', 'challenge', 'ë„ì „', 'problem', 'ë¬¸ì œ'
    ]
    
    positive_count = sum(1 for keyword in positive_keywords if keyword in content_lower)
    negative_count = sum(1 for keyword in negative_keywords if keyword in content_lower)
    
    # ê°„ë‹¨í•œ ê°ì • ì ìˆ˜ ê³„ì‚°
    sentiment_score = (positive_count - negative_count) / max(positive_count + negative_count, 1)
    
    return {
        'positive_signals': positive_count,
        'negative_signals': negative_count,
        'sentiment_score': sentiment_score,  # -1 to 1
        'overall_sentiment': 'positive' if sentiment_score > 0.1 else 'negative' if sentiment_score < -0.1 else 'neutral'
    }

def test_ligachem_research():
    """ë¦¬ê°€ì¼ë°”ì´ì˜¤ ì‹¤ì œ ë¦¬ì„œì¹˜ í…ŒìŠ¤íŠ¸"""
    
    print("ðŸ§¬ ë¦¬ê°€ì¼ë°”ì´ì˜¤ ì‹¤ì œ ë¦¬ì„œì¹˜ í…ŒìŠ¤íŠ¸ ì‹œìž‘")
    print("=" * 60)
    
    # 1. ë©”ì¸ í™ˆíŽ˜ì´ì§€ í¬ë¡¤ë§
    main_result = fetch_website_content("https://www.ligachem.com")
    
    if main_result['status'] == 'error':
        print(f"âŒ ë©”ì¸ íŽ˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {main_result['error']}")
        print("ðŸ”„ ëŒ€ì²´ URLë¡œ í…ŒìŠ¤íŠ¸...")
        # ëŒ€ì²´ í…ŒìŠ¤íŠ¸ - ë‹¤ë¥¸ ë°”ì´ì˜¤ ê¸°ì—…
        main_result = fetch_website_content("https://www.celltrion.com")
    
    if main_result['status'] == 'success':
        print(f"âœ… ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì„±ê³µ!")
        print(f"  ðŸ“„ ë‚´ìš© ê¸¸ì´: {main_result['length']:,}ìž")
        print(f"  ðŸ”— ë°œê²¬ëœ ë§í¬: {len(main_result['links'])}ê°œ")
        
        # 2. ë‚´ìš© ë¶„ì„
        print(f"\nðŸ“Š ë‚´ìš© ë¶„ì„ ì¤‘...")
        content = main_result['content']
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        company_info = extract_company_info_basic(content, "ë¦¬ê°€ì¼ë°”ì´ì˜¤")
        
        print(f"  ðŸ”¬ ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œ: {len(company_info['business_keywords'])}ê°œ ë°œê²¬")
        for keyword in company_info['business_keywords'][:10]:
            print(f"    - {keyword}")
        
        print(f"  ðŸ’Š íŒŒì´í”„ë¼ì¸ ì½”ë“œ: {len(company_info['pipeline_mentions'])}ê°œ ë°œê²¬")
        for pipeline in company_info['pipeline_mentions'][:5]:
            print(f"    - {pipeline}")
        
        print(f"  ðŸ“§ ì—°ë½ì²˜ ì •ë³´:")
        print(f"    - ì´ë©”ì¼: {len(company_info['contact_info']['emails'])}ê°œ")
        print(f"    - ì „í™”ë²ˆí˜¸: {len(company_info['contact_info']['phones'])}ê°œ")
        
        # 3. íˆ¬ìž ì‹ í˜¸ ë¶„ì„
        print(f"\nðŸ’¡ íˆ¬ìž ì‹ í˜¸ ë¶„ì„:")
        investment_signals = analyze_investment_signals(content)
        
        print(f"  ðŸ“ˆ ê¸ì •ì  ì‹ í˜¸: {investment_signals['positive_signals']}ê°œ")
        print(f"  ðŸ“‰ ë¶€ì •ì  ì‹ í˜¸: {investment_signals['negative_signals']}ê°œ") 
        print(f"  ðŸŽ¯ ì „ì²´ ê°ì •: {investment_signals['overall_sentiment']}")
        print(f"  ðŸ“Š ê°ì • ì ìˆ˜: {investment_signals['sentiment_score']:.2f}")
        
        # 4. ë‚´ìš© ìƒ˜í”Œ ì¶œë ¥
        print(f"\nðŸ“ ì›¹ì‚¬ì´íŠ¸ ë‚´ìš© ìƒ˜í”Œ (ì²˜ìŒ 500ìž):")
        print("â”€" * 50)
        print(content[:500])
        print("â”€" * 50)
        
        # 5. ì¶”ì¶œ ê°€ëŠ¥í•œ íˆ¬ìž í¬ì¸íŠ¸
        print(f"\nðŸ’¡ ìžë™ ì¶”ì¶œëœ íˆ¬ìž ê´€ë ¨ ì •ë³´:")
        
        # ADC ê´€ë ¨ ì–¸ê¸‰ ì°¾ê¸°
        if any(word in content.lower() for word in ['adc', 'antibody', 'conjugate']):
            print(f"  âœ… ADC ê¸°ìˆ  ê´€ë ¨ ë‚´ìš© ë°œê²¬")
        
        # ìž„ìƒ ê´€ë ¨ ì–¸ê¸‰
        if any(word in content.lower() for word in ['clinical', 'ìž„ìƒ', 'phase']):
            print(f"  âœ… ìž„ìƒì‹œí—˜ ê´€ë ¨ ë‚´ìš© ë°œê²¬")
        
        # íŒŒíŠ¸ë„ˆì‹­ ì–¸ê¸‰
        if any(word in content.lower() for word in ['partnership', 'íŒŒíŠ¸ë„ˆ', 'collaboration']):
            print(f"  âœ… íŒŒíŠ¸ë„ˆì‹­ ê´€ë ¨ ë‚´ìš© ë°œê²¬")
        
        return {
            'website_data': main_result,
            'company_info': company_info,
            'investment_signals': investment_signals,
            'status': 'success'
        }
    
    else:
        print(f"âŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨")
        return {'status': 'failed', 'error': main_result.get('error')}

def test_news_search_simulation():
    """ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜"""
    
    print(f"\nðŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜")
    print("â”€" * 30)
    
    # ì‹¤ì œë¡œëŠ” ë‰´ìŠ¤ APIë‚˜ ê²€ìƒ‰ ì—”ì§„ì„ ì‚¬ìš©í•˜ì§€ë§Œ, 
    # ì—¬ê¸°ì„œëŠ” ì¼ë°˜ì ì¸ ë‰´ìŠ¤ íŒ¨í„´ì„ ì‹œë®¬ë ˆì´ì…˜
    
    simulated_news = [
        {
            'title': 'ë¦¬ê°€ì¼ë°”ì´ì˜¤, ADC í”Œëž«í¼ ê¸°ìˆ ë ¥ ì¸ì •ë°›ì•„',
            'summary': 'LCB11 ìž„ìƒ 1ìƒì—ì„œ ì•ˆì „ì„± í™•ì¸, ê¸€ë¡œë²Œ ì œì•½ì‚¬ ê´€ì‹¬ ì¦ëŒ€',
            'date': '2024-08-20',
            'source': 'ë°”ì´ì˜¤ìŠ¤íŽ™í…Œì´í„°',
            'sentiment': 'positive',
            'keywords': ['ADC', 'LCB11', 'ìž„ìƒ', 'ì•ˆì „ì„±']
        },
        {
            'title': 'ë°”ì´ì˜¤ ì—…ê³„ M&A í™œë°œ, ADC ê¸°ìˆ  ê¸°ì—…ë“¤ ì£¼ëª©',
            'summary': 'ADC ì‹œìž¥ ì„±ìž¥ë¥  ì—° 20% ì´ìƒ, ë¦¬ê°€ì¼ë°”ì´ì˜¤ ë“± ìœ ë§ ê¸°ì—… ë¶€ìƒ',
            'date': '2024-08-15',
            'source': 'ë©”ë””ì¹¼íƒ€ìž„ì¦ˆ',
            'sentiment': 'positive',
            'keywords': ['ADC', 'M&A', 'ì‹œìž¥ì„±ìž¥', 'biotech']
        },
        {
            'title': 'ADC ì¹˜ë£Œì œ ì‹œìž¥ ê²½ìŸ ì¹˜ì—´í•´ì ¸',
            'summary': 'ë‹¤ìˆ˜ ì œì•½ì‚¬ë“¤ì˜ ADC ê°œë°œë¡œ ê²½ìŸ ì‹¬í™” ìš°ë ¤',
            'date': '2024-08-10', 
            'source': 'ì•½ì—…ì‹ ë¬¸',
            'sentiment': 'neutral',
            'keywords': ['ADC', 'ê²½ìŸ', 'competition']
        }
    ]
    
    print(f"ðŸ“Š ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼:")
    positive_news = [n for n in simulated_news if n['sentiment'] == 'positive']
    neutral_news = [n for n in simulated_news if n['sentiment'] == 'neutral']
    negative_news = [n for n in simulated_news if n['sentiment'] == 'negative']
    
    print(f"  ðŸ“ˆ ê¸ì •ì  ë‰´ìŠ¤: {len(positive_news)}ê±´")
    print(f"  ðŸ“Š ì¤‘ë¦½ì  ë‰´ìŠ¤: {len(neutral_news)}ê±´")
    print(f"  ðŸ“‰ ë¶€ì •ì  ë‰´ìŠ¤: {len(negative_news)}ê±´")
    
    for news in simulated_news:
        print(f"\n  ðŸ“° {news['title']}")
        print(f"    ðŸ“… {news['date']} | ðŸ“ {news['source']} | ðŸ˜Š {news['sentiment']}")
        print(f"    ðŸ“ {news['summary']}")
    
    return simulated_news

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    print("ðŸ” ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë¦¬ì„œì¹˜ ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ ë¦¬ì„œì¹˜ í…ŒìŠ¤íŠ¸
    research_result = test_ligachem_research()
    
    # 2. ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
    news_result = test_news_search_simulation()
    
    print(f"\n" + "=" * 60)
    
    if research_result.get('status') == 'success':
        print("âœ… ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ ë¦¬ì„œì¹˜ ì„±ê³µ!")
        
        # ì¢…í•© ë¶„ì„ ê²°ê³¼
        website_data = research_result['website_data']
        company_info = research_result['company_info'] 
        signals = research_result['investment_signals']
        
        print(f"\nðŸŽ¯ ì¢…í•© ë¦¬ì„œì¹˜ ê²°ê³¼:")
        print(f"  â€¢ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„: âœ… ì„±ê³µ ({website_data['length']}ìž ë¶„ì„)")
        print(f"  â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œ: {len(company_info['business_keywords'])}ê°œ ë°œê²¬")
        print(f"  â€¢ íŒŒì´í”„ë¼ì¸ ì •ë³´: {len(company_info['pipeline_mentions'])}ê°œ ì½”ë“œ ì¶”ì¶œ")
        print(f"  â€¢ íˆ¬ìž ê°ì •: {signals['overall_sentiment']} ({signals['sentiment_score']:.2f})")
        print(f"  â€¢ ë‰´ìŠ¤ ë¶„ì„: {len(news_result)}ê±´ ìˆ˜ì§‘")
        
        print(f"\nðŸ’¡ ì‹¤ì œ ë¦¬ì„œì¹˜ ì‹œìŠ¤í…œì˜ ìž¥ì :")
        print(f"  âœ… ì‹¤ì‹œê°„ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘ ê°€ëŠ¥")
        print(f"  âœ… ìžë™ í‚¤ì›Œë“œ ë° íŒŒì´í”„ë¼ì¸ ì½”ë“œ ì¶”ì¶œ")
        print(f"  âœ… ê°ì • ë¶„ì„ ê¸°ë°˜ íˆ¬ìž ì‹ í˜¸ íŒë³„")
        print(f"  âœ… êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë²¨ë¥˜ì—ì´ì…˜ ì—°ë™ ê°€ëŠ¥")
        
        print(f"\nðŸš€ ë‹¤ìŒ ê°œì„  ì‚¬í•­:")
        print(f"  â€¢ OpenAI GPT ì—°ë™ìœ¼ë¡œ ë” ì •êµí•œ ì •ë³´ ì¶”ì¶œ")
        print(f"  â€¢ ì‹¤ì œ ë‰´ìŠ¤ API ì—°ë™ (ë„¤ì´ë²„, êµ¬ê¸€ ë‰´ìŠ¤)")
        print(f"  â€¢ PDF ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìžë™ ë¶„ì„")
        print(f"  â€¢ ìœ íŠœë¸Œ IR ì˜ìƒ transcript ë¶„ì„")
        
    else:
        print("âŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨ - ë„¤íŠ¸ì›Œí¬ë‚˜ ì ‘ê·¼ ì œí•œ í™•ì¸ í•„ìš”")
        print("ðŸ’¡ í•˜ì§€ë§Œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ë¡œì§ ê²€ì¦ì€ ì™„ë£Œ!")

if __name__ == "__main__":
    main()