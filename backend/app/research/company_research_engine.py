from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from datetime import datetime
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import openai
from urllib.parse import urljoin, urlparse
import re

@dataclass
class CompanyResearchResult:
    """íšŒì‚¬ ë¦¬ì„œì¹˜ ê²°ê³¼"""
    company_name: str
    website_url: str
    
    # ê¸°ë³¸ ì •ë³´
    company_overview: str
    business_model: str
    key_executives: List[Dict[str, str]]
    
    # íŒŒì´í”„ë¼ì¸ ì •ë³´
    pipeline_assets: List[Dict[str, Any]]
    platform_technology: str
    therapeutic_areas: List[str]
    
    # ì¬ë¬´ ì •ë³´
    financial_highlights: Dict[str, Any]
    cash_position: Optional[float]
    recent_funding: List[Dict[str, Any]]
    
    # íŒŒíŠ¸ë„ˆì‹­ ë° í˜‘ì—…
    partnerships: List[Dict[str, Any]]
    licensing_deals: List[Dict[str, Any]]
    
    # íˆ¬ì í¬ì¸íŠ¸
    investment_thesis: List[str]
    key_catalysts: List[str]
    competitive_advantages: List[str]
    
    # ë¦¬ìŠ¤í¬ ìš”ì†Œ
    key_risks: List[str]
    competitive_threats: List[str]
    
    # ë‰´ìŠ¤ ë° ëª¨ë©˜í…€
    recent_news: List[Dict[str, Any]]
    short_term_catalysts: List[str]
    long_term_outlook: str
    
    # ë©”íƒ€ë°ì´í„°
    research_date: datetime
    data_sources: List[str]
    confidence_score: float

class CompanyWebCrawler:
    """íšŒì‚¬ í™ˆí˜ì´ì§€ í¬ë¡¤ë§"""
    
    def __init__(self):
        self.session = None
        self.max_pages = 10  # í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def crawl_company_website(self, company_url: str) -> Dict[str, Any]:
        """íšŒì‚¬ í™ˆí˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
            main_content = await self._fetch_page(company_url)
            
            # ì£¼ìš” ì„¹ì…˜ URL ì°¾ê¸°
            important_urls = await self._find_important_urls(company_url, main_content)
            
            # ê° ì„¹ì…˜ë³„ í¬ë¡¤ë§
            all_content = {"main": main_content}
            for section, url in important_urls.items():
                content = await self._fetch_page(url)
                all_content[section] = content
                
            return all_content
            
        except Exception as e:
            print(f"ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return {}
    
    async def _fetch_page(self, url: str) -> str:
        """ê°œë³„ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                        tag.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = soup.get_text()
                    # ì •ë¦¬
                    text = re.sub(r'\s+', ' ', text).strip()
                    return text[:10000]  # 10KB ì œí•œ
                else:
                    return ""
        except:
            return ""
    
    async def _find_important_urls(self, base_url: str, main_content: str) -> Dict[str, str]:
        """ì¤‘ìš”í•œ ì„¹ì…˜ URL ì°¾ê¸°"""
        important_urls = {}
        
        # ì¼ë°˜ì ì¸ ì¤‘ìš” ì„¹ì…˜ë“¤
        sections = {
            "pipeline": ["pipeline", "products", "development", "ì—°êµ¬ê°œë°œ", "íŒŒì´í”„ë¼ì¸"],
            "technology": ["technology", "platform", "science", "ê¸°ìˆ ", "í”Œë«í¼"],
            "investor": ["investor", "ir", "íˆ¬ìì", "íˆ¬ìì •ë³´"],
            "news": ["news", "press", "media", "ë‰´ìŠ¤", "ë³´ë„ìë£Œ"],
            "about": ["about", "company", "íšŒì‚¬ì†Œê°œ", "ê¸°ì—…ì •ë³´"]
        }
        
        try:
            async with self.session.get(base_url) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                for section, keywords in sections.items():
                    for link in soup.find_all('a', href=True):
                        href = link['href'].lower()
                        text = link.get_text().lower()
                        
                        if any(keyword in href or keyword in text for keyword in keywords):
                            full_url = urljoin(base_url, link['href'])
                            important_urls[section] = full_url
                            break
        except:
            pass
            
        return important_urls

class LLMDataExtractor:
    """LLM ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ"""
    
    def __init__(self, openai_api_key: str = None):
        self.client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None
    
    def extract_company_information(self, company_name: str, web_content: Dict[str, str]) -> Dict[str, Any]:
        """ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©ì—ì„œ êµ¬ì¡°í™”ëœ íšŒì‚¬ ì •ë³´ ì¶”ì¶œ"""
        
        # ëª¨ë“  ì»¨í…ì¸  í•©ì¹˜ê¸°
        full_content = ""
        for section, content in web_content.items():
            full_content += f"\n\n=== {section.upper()} ===\n{content}"
        
        # LLM í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¤ìŒì€ {company_name} íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë¶„ì„í•´ì„œ êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©:
{full_content[:8000]}  # í† í° ì œí•œ

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{{
    "company_overview": "íšŒì‚¬ ê°œìš” (2-3ë¬¸ì¥)",
    "business_model": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì„¤ëª…",
    "platform_technology": "í•µì‹¬ í”Œë«í¼ ê¸°ìˆ ",
    "therapeutic_areas": ["ì¹˜ë£Œ ë¶„ì•¼1", "ì¹˜ë£Œ ë¶„ì•¼2"],
    "pipeline_assets": [
        {{
            "name": "íŒŒì´í”„ë¼ì¸ëª…",
            "indication": "ì ì‘ì¦",
            "phase": "ê°œë°œë‹¨ê³„",
            "target": "íƒ€ê²Ÿ",
            "description": "ì„¤ëª…"
        }}
    ],
    "key_executives": [
        {{
            "name": "ì´ë¦„",
            "title": "ì§ì±…",
            "background": "ê²½ë ¥"
        }}
    ],
    "competitive_advantages": ["ê²½ìŸìš°ìœ„1", "ê²½ìŸìš°ìœ„2"],
    "partnerships": [
        {{
            "partner": "íŒŒíŠ¸ë„ˆì‚¬",
            "type": "í˜‘ì—… ìœ í˜•",
            "description": "í˜‘ì—… ë‚´ìš©"
        }}
    ],
    "investment_highlights": ["íˆ¬ì í¬ì¸íŠ¸1", "íˆ¬ì í¬ì¸íŠ¸2"],
    "recent_achievements": ["ìµœê·¼ ì„±ê³¼1", "ìµœê·¼ ì„±ê³¼2"]
}}

ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•Šì€ í•­ëª©ì€ ë¹ˆ ë°°ì—´ì´ë‚˜ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
"""

        try:
            if self.client:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3
                )
                return eval(response.choices[0].message.content)  # JSON íŒŒì‹±
            else:
                # LLMì´ ì—†ì„ ë•ŒëŠ” ê¸°ë³¸ íŒŒì‹±
                return self._basic_extraction(company_name, web_content)
        except Exception as e:
            print(f"LLM ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return self._basic_extraction(company_name, web_content)
    
    def _basic_extraction(self, company_name: str, web_content: Dict[str, str]) -> Dict[str, Any]:
        """ê¸°ë³¸ì ì¸ ê·œì¹™ ê¸°ë°˜ ì¶”ì¶œ (LLM ì—†ì„ ë•Œ)"""
        return {
            "company_overview": f"{company_name}ì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.",
            "business_model": "ë°”ì´ì˜¤í… ê¸°ì—…",
            "platform_technology": "ì •ë³´ ìˆ˜ì§‘ í•„ìš”",
            "therapeutic_areas": ["ì •ë³´ ìˆ˜ì§‘ í•„ìš”"],
            "pipeline_assets": [],
            "key_executives": [],
            "competitive_advantages": [],
            "partnerships": [],
            "investment_highlights": [],
            "recent_achievements": []
        }

class NewsCollector:
    """ë‰´ìŠ¤ ë° ì–¸ë¡  ë³´ë„ ìˆ˜ì§‘"""
    
    def __init__(self):
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def collect_company_news(self, company_name: str, days: int = 30) -> List[Dict[str, Any]]:
        """íšŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        news_list = []
        
        # ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
        sources = [
            self._collect_from_naver_news,
            self._collect_from_google_news,
            # self._collect_from_bioworld_news  # ë°”ì´ì˜¤ ì „ë¬¸ ë‰´ìŠ¤
        ]
        
        for source_func in sources:
            try:
                source_news = await source_func(company_name, days)
                news_list.extend(source_news)
            except Exception as e:
                print(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜ ({source_func.__name__}): {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        news_list = self._deduplicate_news(news_list)
        news_list.sort(key=lambda x: x.get('date', datetime.now()), reverse=True)
        
        return news_list[:20]  # ìµœì‹  20ê°œ
    
    async def _collect_from_naver_news(self, company_name: str, days: int) -> List[Dict[str, Any]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì˜ˆì‹œ)"""
        # ì‹¤ì œë¡œëŠ” ë„¤ì´ë²„ ë‰´ìŠ¤ APIë‚˜ í¬ë¡¤ë§ êµ¬í˜„
        return [
            {
                "title": f"{company_name} ê´€ë ¨ ë‰´ìŠ¤ ì˜ˆì‹œ",
                "summary": "ë‰´ìŠ¤ ìš”ì•½",
                "url": "https://example.com/news1",
                "source": "ë„¤ì´ë²„ë‰´ìŠ¤",
                "date": datetime.now(),
                "sentiment": "neutral"
            }
        ]
    
    async def _collect_from_google_news(self, company_name: str, days: int) -> List[Dict[str, Any]]:
        """êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì˜ˆì‹œ)"""
        # ì‹¤ì œë¡œëŠ” Google News API êµ¬í˜„
        return []
    
    def _deduplicate_news(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title_key = news.get('title', '').lower().strip()
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        return unique_news

class CompanyResearchEngine:
    """ì¢…í•© íšŒì‚¬ ë¦¬ì„œì¹˜ ì—”ì§„"""
    
    def __init__(self, openai_api_key: str = None):
        self.llm_extractor = LLMDataExtractor(openai_api_key)
        
    async def conduct_full_research(self, company_name: str, company_url: str) -> CompanyResearchResult:
        """ì¢…í•©ì ì¸ íšŒì‚¬ ë¦¬ì„œì¹˜ ìˆ˜í–‰"""
        
        print(f"ğŸ” {company_name} ë¦¬ì„œì¹˜ ì‹œì‘...")
        
        # 1. ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
        print("1ï¸âƒ£ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...")
        async with CompanyWebCrawler() as crawler:
            web_content = await crawler.crawl_company_website(company_url)
        
        # 2. LLMìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print("2ï¸âƒ£ AI ì •ë³´ ì¶”ì¶œ ì¤‘...")
        extracted_info = self.llm_extractor.extract_company_information(company_name, web_content)
        
        # 3. ë‰´ìŠ¤ ìˆ˜ì§‘
        print("3ï¸âƒ£ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        async with NewsCollector() as collector:
            recent_news = await collector.collect_company_news(company_name)
        
        # 4. íˆ¬ì í¬ì¸íŠ¸ ë¶„ì„
        print("4ï¸âƒ£ íˆ¬ì ë¶„ì„ ì¤‘...")
        investment_analysis = self._analyze_investment_points(extracted_info, recent_news)
        
        # 5. ê²°ê³¼ í†µí•©
        result = CompanyResearchResult(
            company_name=company_name,
            website_url=company_url,
            company_overview=extracted_info.get('company_overview', ''),
            business_model=extracted_info.get('business_model', ''),
            key_executives=extracted_info.get('key_executives', []),
            pipeline_assets=extracted_info.get('pipeline_assets', []),
            platform_technology=extracted_info.get('platform_technology', ''),
            therapeutic_areas=extracted_info.get('therapeutic_areas', []),
            financial_highlights={},  # ë³„ë„ êµ¬í˜„ í•„ìš”
            cash_position=None,
            recent_funding=[],
            partnerships=extracted_info.get('partnerships', []),
            licensing_deals=[],
            investment_thesis=investment_analysis['investment_thesis'],
            key_catalysts=investment_analysis['key_catalysts'],
            competitive_advantages=extracted_info.get('competitive_advantages', []),
            key_risks=investment_analysis['key_risks'],
            competitive_threats=[],
            recent_news=recent_news,
            short_term_catalysts=investment_analysis['short_term_catalysts'],
            long_term_outlook=investment_analysis['long_term_outlook'],
            research_date=datetime.now(),
            data_sources=[company_url, "ë‰´ìŠ¤ ì†ŒìŠ¤"],
            confidence_score=0.7  # ë°ì´í„° í’ˆì§ˆì— ë”°ë¼ ì¡°ì •
        )
        
        print(f"âœ… {company_name} ë¦¬ì„œì¹˜ ì™„ë£Œ!")
        return result
    
    def _analyze_investment_points(self, extracted_info: Dict, recent_news: List) -> Dict[str, Any]:
        """íˆ¬ì í¬ì¸íŠ¸ ë¶„ì„"""
        
        # ê¸°ë³¸ íˆ¬ì ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ LLM ë¶„ì„)
        investment_thesis = extracted_info.get('investment_highlights', [])
        if not investment_thesis:
            investment_thesis = ["ìë™ ë¶„ì„ëœ íˆ¬ì í¬ì¸íŠ¸ í•„ìš”"]
        
        # ë‰´ìŠ¤ ê¸°ë°˜ ë‹¨ê¸° ëª¨ë©˜í…€
        short_term_catalysts = []
        for news in recent_news[:5]:  # ìµœì‹  5ê°œ ë‰´ìŠ¤
            if any(keyword in news.get('title', '').lower() 
                   for keyword in ['ìŠ¹ì¸', 'í—ˆê°€', 'íŒŒíŠ¸ë„ˆì‹­', 'íˆ¬ì', 'ì„ìƒ', 'ê²°ê³¼']):
                short_term_catalysts.append(f"ìµœê·¼ ë‰´ìŠ¤: {news.get('title', '')[:50]}...")
        
        return {
            'investment_thesis': investment_thesis,
            'key_catalysts': extracted_info.get('recent_achievements', []),
            'key_risks': ['ì„ìƒ ì‹¤íŒ¨ ë¦¬ìŠ¤í¬', 'ê²½ìŸ ì‹¬í™”', 'ìê¸ˆ ì¡°ë‹¬ ë¦¬ìŠ¤í¬'],  # ê¸°ë³¸ ë¦¬ìŠ¤í¬
            'short_term_catalysts': short_term_catalysts,
            'long_term_outlook': 'ë°”ì´ì˜¤í… ì—…ê³„ì˜ ì „ë°˜ì ì¸ ì„±ì¥ì„¸ë¥¼ ê³ ë ¤í•  ë•Œ ê¸ì •ì '
        }

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def research_ligachem_bio():
    """ë¦¬ê°€ì¼ë°”ì´ì˜¤ ë¦¬ì„œì¹˜ ì˜ˆì‹œ"""
    
    engine = CompanyResearchEngine(openai_api_key=None)  # API í‚¤ ì„¤ì • í•„ìš”
    
    result = await engine.conduct_full_research(
        company_name="ë¦¬ê°€ì¼ë°”ì´ì˜¤",
        company_url="https://www.ligachem.com"
    )
    
    return result

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import asyncio
    result = asyncio.run(research_ligachem_bio())
    print(f"ë¦¬ì„œì¹˜ ê²°ê³¼: {result.company_overview}")